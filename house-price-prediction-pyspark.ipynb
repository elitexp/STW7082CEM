{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/sujip/anaconda3/envs/assignment/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/sujip/anaconda3/envs/assignment/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/30 17:40:18 WARN Utils: Your hostname, Sujips-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.18.6 instead (on interface en0)\n",
      "24/06/30 17:40:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/30 17:40:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.18.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>House Price Prediction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112b1bb20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"House Price Prediction\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----+----+---------+------+-------------+----------+--------+--------+--------+--------+---+--------+------------+---------+-------------+--------------------+-------------+\n",
      "|           Suburb|          Address|Rooms|Type|    Price|Method|      SellerG|      Date|Distance|Postcode|Bedroom2|Bathroom|Car|Landsize|BuildingArea|YearBuilt|  CouncilArea|          Regionname|Propertycount|\n",
      "+-----------------+-----------------+-----+----+---------+------+-------------+----------+--------+--------+--------+--------+---+--------+------------+---------+-------------+--------------------+-------------+\n",
      "|       Aberfeldie|   241 Buckley St|    4|   h|1380000.0|    VB|       Nelson|12/08/2017|     7.5|  3040.0|     4.0|     2.0|2.0|   766.0|        NULL|     NULL|Moonee Valley|Western Metropolitan|       1543.0|\n",
      "|        Northcote|    67 Charles St|    2|   h|1100000.0|    SP|       Jellis|20/05/2017|     5.5|  3070.0|     2.0|     1.0|1.0|   189.0|        NULL|     NULL|      Darebin|Northern Metropol...|      11364.0|\n",
      "|     Balwyn North|       42 Maud St|    3|   h|1480000.0|    PI|       Jellis|15/10/2016|     9.2|  3104.0|     3.0|     1.0|4.0|   605.0|       116.0|   1950.0|   Boroondara|Southern Metropol...|       7809.0|\n",
      "|        Brunswick|      13 Percy St|    3|   h|1055000.0|     S|       Nelson| 7/05/2016|     5.2|  3056.0|     3.0|     1.0|1.0|   324.0|        NULL|   1930.0|     Moreland|Northern Metropol...|      11918.0|\n",
      "|Templestowe Lower| 253 Thompsons Rd|    4|   h|1000000.0|    VB|hockingstuart|13/08/2016|    13.8|  3107.0|     4.0|     3.0|2.0|   728.0|       164.0|   1970.0|   Manningham|Eastern Metropolitan|       5420.0|\n",
      "|           Coburg|4/34 Gladstone St|    2|   u| 650000.0|     S|        Barry|29/04/2017|     7.8|  3058.0|     2.0|     1.0|1.0|   136.0|        81.0|   1975.0|     Moreland|Northern Metropol...|      11204.0|\n",
      "|        Glen Iris|     27 Pascoe St|    3|   h|1510000.0|    PI|hockingstuart|10/09/2016|     9.2|  3146.0|     3.0|     2.0|2.0|   370.0|        NULL|     NULL|   Boroondara|Southern Metropol...|      10412.0|\n",
      "|      Pascoe Vale|   5/10 Dorset Rd|    3|   u| 600000.0|     S|       Nelson| 4/03/2017|     9.9|  3044.0|     3.0|     1.0|1.0|   204.0|       124.0|   2008.0|     Moreland|Northern Metropol...|       7485.0|\n",
      "|          Preston|       23 Dean St|    4|   h|1260000.0|    SP|       Nelson|14/05/2016|     8.8|  3072.0|     4.0|     3.0|2.0|   654.0|       150.0|   1950.0|      Darebin|Northern Metropol...|      14577.0|\n",
      "|    Sunshine West|       26 Bell St|    4|   h| 645400.0|     S|        Barry| 7/11/2016|    13.5|  3020.0|     4.0|     2.0|3.0|   765.0|        NULL|     NULL|     Brimbank|Western Metropolitan|       6763.0|\n",
      "|           Coburg|       24 Webb St|    3|   h| 870000.0|     S|       Nelson|22/08/2016|     7.8|  3058.0|     3.0|     1.0|1.0|   371.0|       100.0|   1980.0|     Moreland|Northern Metropol...|      11204.0|\n",
      "|     Malvern East|   2/25 Fisher St|    3|   t|1135000.0|     S|       Jellis|17/09/2016|    11.2|  3145.0|     3.0|     3.0|2.0|     0.0|       130.0|   1988.0|  Stonnington|Southern Metropol...|       8801.0|\n",
      "|     Coburg North|   27 Williams Rd|    4|   h| 770000.0|     S|         Brad|13/05/2017|     9.2|  3058.0|     4.0|     1.0|1.0|   545.0|        NULL|     NULL|     Moreland|Northern Metropol...|       3445.0|\n",
      "|      Keilor East|      83 Wyong St|    3|   h| 785000.0|     S|        Barry|18/06/2016|    12.8|  3033.0|     3.0|     1.0|2.0|   606.0|       141.0|   1965.0|Moonee Valley|Western Metropolitan|       5629.0|\n",
      "|          Burwood|1/392 Burwood Hwy|    3|   t| 752000.0|     S|       Allens|17/09/2016|    11.7|  3125.0|     4.0|     2.0|2.0|   217.0|        NULL|   1993.0|   Whitehorse|Southern Metropol...|       5678.0|\n",
      "|       Ascot Vale|     62 Mirams St|    3|   h| 890000.0|    SP|         Brad|16/07/2016|     5.9|  3032.0|     3.0|     1.0|1.0|   242.0|       110.0|   1920.0|Moonee Valley|Western Metropolitan|       6567.0|\n",
      "|    Hawthorn East|   1/27 Auburn Gr|    3|   h|1405000.0|     S|          Kay|20/05/2017|     7.5|  3123.0|     3.0|     2.0|2.0|   185.0|       185.0|   1985.0|   Boroondara|Southern Metropol...|       6482.0|\n",
      "|          Glenroy| 73 Augustine Tce|    4|   h| 651000.0|    PI|        Eview|27/05/2017|    11.2|  3046.0|     4.0|     1.0|2.0|   587.0|        NULL|     NULL|     Moreland|Northern Metropol...|       8870.0|\n",
      "|        Spotswood|       59 Hope St|    4|   h| 871500.0|    PI|          Jas|28/08/2016|     7.7|  3015.0|     4.0|     2.0|0.0|   389.0|       158.0|   1990.0|  Hobsons Bay|Western Metropolitan|       1223.0|\n",
      "|           Elwood|   5/61 Ormond Rd|    2|   u| 572500.0|    PI|          Kay|28/08/2016|     7.7|  3184.0|     2.0|     2.0|1.0|  1307.0|        NULL|   1960.0| Port Phillip|Southern Metropol...|       8989.0|\n",
      "+-----------------+-----------------+-----+----+---------+------+-------------+----------+--------+--------+--------+--------+---+--------+------------+---------+-------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace 'train.csv' and 'test.csv' with the actual paths to your datasets\n",
    "train_data = spark.read.csv('housing-house-prediction/train_set.csv', header=True, inferSchema=True)\n",
    "test_data = spark.read.csv('housing-house-prediction/test_set.csv', header=True, inferSchema=True)\n",
    "train_data=train_data.drop('index','Lattitude','Longtitude')\n",
    "test_data=test_data.drop('index','Lattitude','Longtitude')\n",
    "\n",
    "# Show the first few rows of the training data\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when\n",
    "\n",
    "# Fill missing values with the median for numerical columns\n",
    "numerical_columns = [col for col, dtype in train_data.dtypes if dtype in ('int', 'double')]\n",
    "for column in numerical_columns:\n",
    "    median_value = train_data.approxQuantile(column, [0.5], 0.25)[0]\n",
    "    train_data = train_data.fillna({column: median_value})\n",
    "\n",
    "# Fill missing values with the median for numerical columns\n",
    "numerical_columns = [col for col, dtype in test_data.dtypes if dtype in ('int', 'double')]\n",
    "for column in numerical_columns:\n",
    "    median_value = test_data.approxQuantile(column, [0.5], 0.25)[0]\n",
    "    test_data = test_data.fillna({column: median_value})\n",
    "\n",
    "# Fill missing values with the mode for categorical columns\n",
    "categorical_columns = [col for col, dtype in train_data.dtypes if dtype == 'string']\n",
    "for column in categorical_columns:\n",
    "    mode_value = train_data.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
    "    if mode_value is not None:\n",
    "        test_data = test_data.fillna(mode_value, subset=[column])\n",
    "\n",
    "# Fill missing values with the mode for categorical columns\n",
    "categorical_columns = [col for col, dtype in test_data.dtypes if dtype == 'string']\n",
    "for column in categorical_columns:\n",
    "    mode_value = test_data.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
    "    if mode_value is not None:\n",
    "        test_data = test_data.fillna(mode_value, subset=[column])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Method: string (nullable = true)\n",
      " |-- SellerG: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Postcode: double (nullable = true)\n",
      " |-- Bedroom2: double (nullable = true)\n",
      " |-- Bathroom: double (nullable = true)\n",
      " |-- Car: double (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- BuildingArea: double (nullable = true)\n",
      " |-- YearBuilt: double (nullable = true)\n",
      " |-- CouncilArea: string (nullable = true)\n",
      " |-- Lattitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Regionname: string (nullable = true)\n",
      " |-- Propertycount: double (nullable = true)\n",
      "\n",
      "Number of records: 5432\n",
      "root\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Method: string (nullable = true)\n",
      " |-- SellerG: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Postcode: double (nullable = true)\n",
      " |-- Bedroom2: double (nullable = true)\n",
      " |-- Bathroom: double (nullable = true)\n",
      " |-- Car: double (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- BuildingArea: double (nullable = true)\n",
      " |-- YearBuilt: double (nullable = true)\n",
      " |-- CouncilArea: string (nullable = true)\n",
      " |-- Lattitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Regionname: string (nullable = true)\n",
      " |-- Propertycount: double (nullable = true)\n",
      "\n",
      "Number of records: 8148\n"
     ]
    }
   ],
   "source": [
    "train_data.printSchema()\n",
    "print(f\"Number of records: {train_data.count()}\")\n",
    "test_data.printSchema()\n",
    "print(f\"Number of records: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+\n",
      "|          Regionname|    Price|count|\n",
      "+--------------------+---------+-----+\n",
      "|Northern Metropol...| 825000.0|    3|\n",
      "|South-Eastern Met...| 620000.0|    1|\n",
      "|Southern Metropol...| 465000.0|    2|\n",
      "|Western Metropolitan|1336000.0|    1|\n",
      "|Eastern Metropolitan|1207000.0|    1|\n",
      "|Northern Metropol...| 416000.0|    1|\n",
      "|Northern Metropol...| 436000.0|    1|\n",
      "|South-Eastern Met...|1255000.0|    1|\n",
      "|    Eastern Victoria| 872000.0|    1|\n",
      "|Western Metropolitan| 895000.0|    2|\n",
      "|Eastern Metropolitan|1070000.0|    1|\n",
      "|Northern Metropol...|1600000.0|    5|\n",
      "|Northern Metropol...| 432500.0|    1|\n",
      "|Northern Metropol...|1065000.0|    4|\n",
      "|Northern Metropol...| 756000.0|    1|\n",
      "|Southern Metropol...| 706000.0|    2|\n",
      "|Northern Metropol...|2775000.0|    1|\n",
      "|Northern Metropol...| 612000.0|    1|\n",
      "|Southern Metropol...| 420000.0|    5|\n",
      "|Southern Metropol...| 447000.0|    2|\n",
      "+--------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_wise = train_data.groupBy('Regionname','Price').count()\n",
    "region_wise.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+\n",
      "|Method|    Price|count|\n",
      "+------+---------+-----+\n",
      "|     S| 645000.0|    6|\n",
      "|     S| 695000.0|    8|\n",
      "|     S| 875000.0|    9|\n",
      "|     S|1152500.0|    1|\n",
      "|     S| 445000.0|    4|\n",
      "|     S|1260000.0|   12|\n",
      "|     S|1036000.0|    1|\n",
      "|     S|1715000.0|    3|\n",
      "|     S| 672000.0|    1|\n",
      "|     S| 525500.0|    3|\n",
      "|    PI| 902000.0|    1|\n",
      "|    PI| 655000.0|    2|\n",
      "|    VB|2475000.0|    1|\n",
      "|    SP| 638000.0|    1|\n",
      "|     S| 456000.0|    1|\n",
      "|     S|2635000.0|    1|\n",
      "|    PI| 470000.0|    2|\n",
      "|     S| 385000.0|    2|\n",
      "|     S| 442000.0|    2|\n",
      "|     S| 260000.0|    1|\n",
      "+------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "method_wise = train_data.groupBy('Method','Price').count()\n",
    "method_wise.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check If any Column has null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Suburb' has 0 null values.\n",
      "Column 'Address' has 0 null values.\n",
      "Column 'Rooms' has 0 null values.\n",
      "Column 'Type' has 0 null values.\n",
      "Column 'Price' has 0 null values.\n",
      "Column 'Method' has 0 null values.\n",
      "Column 'SellerG' has 0 null values.\n",
      "Column 'Date' has 0 null values.\n",
      "Column 'Distance' has 0 null values.\n",
      "Column 'Postcode' has 0 null values.\n",
      "Column 'Bedroom2' has 0 null values.\n",
      "Column 'Bathroom' has 0 null values.\n",
      "Column 'Car' has 25 null values.\n",
      "Column 'Landsize' has 0 null values.\n",
      "Column 'BuildingArea' has 2542 null values.\n",
      "Column 'YearBuilt' has 2130 null values.\n",
      "Column 'CouncilArea' has 553 null values.\n",
      "Column 'Lattitude' has 0 null values.\n",
      "Column 'Longtitude' has 0 null values.\n",
      "Column 'Regionname' has 0 null values.\n",
      "Column 'Propertycount' has 0 null values.\n"
     ]
    }
   ],
   "source": [
    "null_counts = {}\n",
    "for col_name in train_data.columns:\n",
    "    null_count = train_data.filter(col(col_name).isNull()).count()\n",
    "    null_counts[col_name] = null_count\n",
    "\n",
    "# Display the null counts\n",
    "for col_name, null_count in null_counts.items():\n",
    "    print(f\"Column '{col_name}' has {null_count} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Suburb: string, Address: string, Rooms: int, Type: string, Price: double, Method: string, SellerG: string, Date: string, Distance: double, Postcode: double, Bedroom2: double, Bathroom: double, Car: double, Landsize: double, BuildingArea: double, YearBuilt: double, CouncilArea: string, Regionname: string, Propertycount: double, Suburb_index: double, Address_index: double, Type_index: double, Method_index: double, SellerG_index: double, Date_index: double, CouncilArea_index: double, Regionname_index: double, Suburb_encoded: vector, Address_encoded: vector, Type_encoded: vector, Method_encoded: vector, SellerG_encoded: vector, Date_encoded: vector, CouncilArea_encoded: vector, Regionname_encoded: vector]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, isnan, when\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "# Encode categorical variables\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid=\"keep\") for column in categorical_columns]\n",
    "encoders = [OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_encoded\") for column in categorical_columns]\n",
    "\n",
    "# Create a pipeline for indexing and encoding\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "# Transform training and test data using the fitted pipeline\n",
    "train_data_final = pipeline_model.transform(train_data)\n",
    "test_data_final = pipeline_model.transform(train_data)\n",
    "train_data_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+--------+--------+--------+---+--------+------------+---------+-------------+\n",
      "|Rooms|    Price|Distance|Postcode|Bedroom2|Bathroom|Car|Landsize|BuildingArea|YearBuilt|Propertycount|\n",
      "+-----+---------+--------+--------+--------+--------+---+--------+------------+---------+-------------+\n",
      "|    4|1380000.0|     7.5|  3040.0|     4.0|     2.0|2.0|   766.0|        92.0|   1941.0|       1543.0|\n",
      "|    2|1100000.0|     5.5|  3070.0|     2.0|     1.0|1.0|   189.0|        92.0|   1941.0|      11364.0|\n",
      "|    3|1480000.0|     9.2|  3104.0|     3.0|     1.0|4.0|   605.0|       116.0|   1950.0|       7809.0|\n",
      "|    3|1055000.0|     5.2|  3056.0|     3.0|     1.0|1.0|   324.0|        92.0|   1930.0|      11918.0|\n",
      "|    4|1000000.0|    13.8|  3107.0|     4.0|     3.0|2.0|   728.0|       164.0|   1970.0|       5420.0|\n",
      "+-----+---------+--------+--------+--------+--------+---+--------+------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop original categorical columns\n",
    "train_data = train_data.drop(*categorical_columns)\n",
    "test_data = test_data.drop(*categorical_columns)\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, percentile_approx\n",
    "\n",
    "def remove_outliers(df, numeric_columns):\n",
    "    # Calculate Q1 and Q3\n",
    "    quantiles = df.approxQuantile(numeric_columns, [0.25, 0.75], 0.05)\n",
    "    \n",
    "    # Create a list of conditions to filter out outliers\n",
    "    conditions = []\n",
    "    for i, column in enumerate(numeric_columns):\n",
    "        Q1 = quantiles[i][0]\n",
    "        Q3 = quantiles[i][1]\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        conditions.append((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "    \n",
    "    # Combine all conditions\n",
    "    combined_condition = conditions[0]\n",
    "    for condition in conditions[1:]:\n",
    "        combined_condition = combined_condition & condition\n",
    "    \n",
    "    # Filter out outliers\n",
    "    df_filtered = df.filter(combined_condition)\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [col_name for col_name, dtype in train_data.dtypes if dtype in ['int', 'double']]\n",
    "\n",
    "train_data_filtered = remove_outliers(train_data_final, numeric_columns)\n",
    "test_data_filtered = remove_outliers(test_data_final, numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3452"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_filtered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assemble features and scale them\n",
    "assembler = VectorAssembler(inputCols=[col+\"_encoded\" for col in categorical_columns] + numerical_columns, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Transform the data to have a features column\n",
    "train_data_assembled = assembler.transform(train_data_filtered)\n",
    "test_data_assembled = assembler.transform(test_data_filtered)\n",
    "\n",
    "scaler_model = scaler.fit(train_data_assembled)\n",
    "\n",
    "train_data_scaled = scaler_model.transform(train_data_assembled)\n",
    "test_data_scaled = scaler_model.transform(test_data_assembled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "['Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Propertycount']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "\n",
    "# Select numerical columns only\n",
    "df_numerical = train_data_filtered.select(numerical_columns + ['Price'])\n",
    "\n",
    "# Assemble all numerical features into a single vector column\n",
    "df_assembler = VectorAssembler(inputCols=numerical_columns, outputCol=\"features\")\n",
    "\n",
    "# Transform the data\n",
    "assembled_df = df_assembler.transform(df_numerical)\n",
    "# ChiSqSelector requires features and a label column\n",
    "selector = ChiSqSelector(featuresCol='features',\n",
    "                         outputCol=\"selected_features\",\n",
    "                         labelCol=\"Price\",\n",
    "                         selectorType=\"numTopFeatures\",\n",
    "                         numTopFeatures=10)\n",
    "\n",
    "# Fit the selector\n",
    "selector_model = selector.fit(assembled_df)\n",
    "\n",
    "# Transform the data to select top features\n",
    "selected_features_df = selector_model.transform(assembled_df)\n",
    "\n",
    "# Print selected feature names\n",
    "selected_feature_indices = selector_model.selectedFeatures\n",
    "print(selected_feature_indices)\n",
    "selected_feature_names = [\n",
    "    numerical_columns[i] for i in selected_feature_indices\n",
    "]\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest\n",
    "import pandas as pd\n",
    "# Perform Chi-Square Test\n",
    "chi_sq_results = ChiSquareTest.test(assembled_df, \"features\", \"Price\").head()\n",
    "\n",
    "# Extract pValues and degreesOfFreedom\n",
    "p_values = chi_sq_results.pValues\n",
    "degrees_of_freedom = chi_sq_results.degreesOfFreedom\n",
    "chi_sq_values = chi_sq_results.statistics\n",
    "\n",
    "# Create a DataFrame with feature names and their chi-squared statistics\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': numerical_columns,\n",
    "    'ChiSqStatistic': chi_sq_values,\n",
    "    'PValue': p_values,\n",
    "    'DegreesOfFreedom': degrees_of_freedom\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by chi-squared statistic in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='ChiSqStatistic', ascending=False)\n",
    "\n",
    "# Export the DataFrame to CSV\n",
    "feature_importance_df.to_csv('selected_features_importance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Names: ['Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Propertycount']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChiSqSelectorModel' object has no attribute 'scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 22\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected Feature Names:\u001b[39m\u001b[38;5;124m\"\u001b[39m, selected_feature_names)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Extract selected features and their importance scores into a pandas DataFrame\u001b[39;00m\n\u001b[1;32m     20\u001b[0m selected_features_importance \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: selected_feature_names,\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance Score\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mselector_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m\n\u001b[1;32m     23\u001b[0m })\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Export the DataFrame to CSV\u001b[39;00m\n\u001b[1;32m     26\u001b[0m selected_features_importance\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselected_features_importance_heatmap.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ChiSqSelectorModel' object has no attribute 'scores'"
     ]
    }
   ],
   "source": [
    "# ChiSqSelector requires features and a label column\n",
    "selector = ChiSqSelector(featuresCol='features',\n",
    "                         outputCol=\"selected_features\",\n",
    "                         labelCol=\"Price\",\n",
    "                         selectorType=\"numTopFeatures\",\n",
    "                         numTopFeatures=10)\n",
    "\n",
    "# Fit the selector\n",
    "selector_model = selector.fit(assembled_df)\n",
    "\n",
    "# Transform the data to select top features\n",
    "selected_features_df = selector_model.transform(assembled_df)\n",
    "\n",
    "# Print selected feature names\n",
    "selected_feature_indices = selector_model.selectedFeatures\n",
    "selected_feature_names = [numerical_columns[i] for i in selected_feature_indices]\n",
    "print(\"Selected Feature Names:\", selected_feature_names)\n",
    "\n",
    "# Extract selected features and their importance scores into a pandas DataFrame\n",
    "selected_features_importance = pd.DataFrame({\n",
    "    'Feature': selected_feature_names,\n",
    "    'Importance Score': selector_model.scores\n",
    "})\n",
    "\n",
    "# Export the DataFrame to CSV\n",
    "selected_features_importance.to_csv('selected_features_importance_heatmap.csv', index=False)\n",
    "\n",
    "print(\"Exported selected features and their importances to 'selected_features_importance.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     scaled_features|\n",
      "+--------------------+\n",
      "|[-0.5230081471560...|\n",
      "|[-0.8068421266191...|\n",
      "|[-1.3134318874330...|\n",
      "|[-0.6020505211837...|\n",
      "|[-0.6559430489298...|\n",
      "|[-0.7529495988729...|\n",
      "|[-0.6128290267329...|\n",
      "|[-0.8319919729006...|\n",
      "|[-0.8679203247314...|\n",
      "|[0.05543831731950...|\n",
      "|[1.43867986280439...|\n",
      "|[1.59317177567673...|\n",
      "|[-0.3325878824528...|\n",
      "|[1.11532469632740...|\n",
      "|[1.10454619077817...|\n",
      "|[-1.3134318874330...|\n",
      "|[-0.3613305639175...|\n",
      "|[-1.3134318874330...|\n",
      "|[-0.6236075322821...|\n",
      "|[0.03028847103795...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+\n",
      "|    Price|\n",
      "+---------+\n",
      "|1097000.0|\n",
      "| 911000.0|\n",
      "| 900000.0|\n",
      "|1000000.0|\n",
      "| 955000.0|\n",
      "|1035000.0|\n",
      "|1172500.0|\n",
      "|1465000.0|\n",
      "|1100000.0|\n",
      "|1195000.0|\n",
      "|1380000.0|\n",
      "|1705000.0|\n",
      "|1330000.0|\n",
      "|1720000.0|\n",
      "|1680000.0|\n",
      "| 507000.0|\n",
      "| 752000.0|\n",
      "| 805000.0|\n",
      "| 440000.0|\n",
      "| 650000.0|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "# Define the features and target\n",
    "features = ['Landsize', 'Propertycount', 'BuildingArea', 'Distance', 'Postcode', 'YearBuilt', 'Rooms', 'Bedroom2', 'Car', 'Bathroom']\n",
    "target = 'Price'\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_data_filtered.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "\n",
    "# Pipeline for assembling and scaling\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Transform training and test data\n",
    "train_transformed = pipeline_model.transform(train_df)\n",
    "test_transformed = pipeline_model.transform(test_df)\n",
    "\n",
    "# Select only the scaled features and target for training and test sets\n",
    "X_train = train_transformed.select(\"scaled_features\")\n",
    "y_train = train_transformed.select(target)\n",
    "X_test = test_transformed.select(\"scaled_features\")\n",
    "y_test = test_transformed.select(target)\n",
    "\n",
    "# Show the transformed features\n",
    "X_train.show()\n",
    "y_train.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_roc_curve(predictions, model_name):\n",
    "    # Extract the probability and label columns\n",
    "    prediction_pdf = predictions.select(\"probability\", \"Price\").toPandas()\n",
    "    prediction_pdf['probability'] = prediction_pdf['probability'].apply(lambda x: x[1])\n",
    "\n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(prediction_pdf['Price'], prediction_pdf['probability'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plotting using Seaborn\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=fpr, y=tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def get_best_model(models, model_names, train_data, test_data):\n",
    "    best_model = None\n",
    "    best_model_name = None\n",
    "    best_r2 = 0.0\n",
    "\n",
    "    # Evaluators for additional metrics\n",
    "    mae_evaluator = RegressionEvaluator(labelCol='Price', predictionCol='prediction', metricName='mae')\n",
    "    r2_evaluator = RegressionEvaluator(labelCol='Price', predictionCol='prediction', metricName='r2')\n",
    "    mse_evaluator = RegressionEvaluator(labelCol='Price', predictionCol='prediction', metricName='mse')\n",
    "    rmse_evaluator = RegressionEvaluator(labelCol='Price', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "    # DataFrame to store results\n",
    "    results = pd.DataFrame(columns=['Model', 'MAE', 'R2', 'MSE', 'RMSE'])\n",
    "    # DataFrame to store results\n",
    "    for model, model_name in zip(models, model_names):\n",
    "        pipeline = Pipeline(stages= [assembler,selector,scaler, model])\n",
    "        pipeline_model = pipeline.fit(train_data)\n",
    "        predictions = pipeline_model.transform(test_data)\n",
    "\n",
    "        mae = mae_evaluator.evaluate(predictions)\n",
    "        r2 = r2_evaluator.evaluate(predictions)\n",
    "        mse = mse_evaluator.evaluate(predictions)\n",
    "        rmse = rmse_evaluator.evaluate(predictions)\n",
    "\n",
    "        print(f\"Metrics for {model_name}:\")\n",
    "        print(f\"MAE: {mae}\")\n",
    "        print(f\"R2: {r2}\")\n",
    "        print(f\"MSE: {mse}\")\n",
    "        print(f\"RMSE: {rmse}\\n\")\n",
    "        # Add results to the DataFrame\n",
    "        new_row = pd.DataFrame({'Model': [model_name],\n",
    "                            'MAE': [mae],\n",
    "                            'R2': [r2],\n",
    "                            'MSE': [mse],\n",
    "                            'RMSE': [rmse]})\n",
    "        results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        # plot_roc_curve(predictions, model_name)\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_model = pipeline_model\n",
    "            best_model_name = model_name\n",
    "            best_r2 = r2\n",
    "\n",
    "    # print(f\"Best model: {best_model_name} with ROC-AUC: {best_roc_auc}\")\n",
    "\n",
    "    # Save the results DataFrame to a CSV file\n",
    "    results.to_csv('model_metrics.csv', index=False)\n",
    "\n",
    "    return best_model, best_model_name, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Suburb: string, Address: string, Rooms: int, Type: string, Price: double, Method: string, SellerG: string, Date: string, Distance: double, Postcode: double, Bedroom2: double, Bathroom: double, Car: double, Landsize: double, BuildingArea: double, YearBuilt: double, CouncilArea: string, Regionname: string, Propertycount: double, Suburb_index: double, Address_index: double, Type_index: double, Method_index: double, SellerG_index: double, Date_index: double, CouncilArea_index: double, Regionname_index: double, Suburb_encoded: vector, Address_encoded: vector, Type_encoded: vector, Method_encoded: vector, SellerG_encoded: vector, Date_encoded: vector, CouncilArea_encoded: vector, Regionname_encoded: vector]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Linear Regression:\n",
      "MAE: 208663.83116561268\n",
      "R2: 0.5401535270619431\n",
      "MSE: 74319747284.15541\n",
      "RMSE: 272616.4838819462\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5v/rs4ws06j7x78q25f1pxgyzd40000gn/T/ipykernel_5732/2324194343.py:66: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Decision Tree Regression:\n",
      "MAE: 191572.69439744233\n",
      "R2: 0.5746793553753022\n",
      "MSE: 68739730939.5033\n",
      "RMSE: 262182.62898121856\n",
      "\n",
      "Metrics for Random Forest Regression:\n",
      "MAE: 190924.73979370054\n",
      "R2: 0.5871825662883217\n",
      "MSE: 66718979384.40486\n",
      "RMSE: 258300.1730243417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of models to evaluate\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "\n",
    "models = [\n",
    "    LinearRegression(featuresCol=\"scaled_features\", labelCol=\"Price\",regParam=0.1),\n",
    "    DecisionTreeRegressor(featuresCol=\"scaled_features\", labelCol=\"Price\"),\n",
    "    RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=\"Price\")\n",
    "]\n",
    "model_names = ['Linear Regression', 'Decision Tree Regression', 'Random Forest Regression']\n",
    "# Get the best model\n",
    "train_df_sampled = train_df.sample(withReplacement=False, fraction=0.34, seed=42)\n",
    "\n",
    "best_model, best_model_name, results = get_best_model(models, model_names, train_df, test_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
